# Deepfake-Detection-using-LSTM-and-ResNext

# Introduction

Deepfake technology, powered by advanced deep learning algorithms, has emerged as a significant threat to modern society by enabling the creation of highly convincing forged media. Deep Fakes manipulate audiovisual content, often superimposing individuals' faces onto different bodies or generating fabricated speeches, raising concerns about misinformation, privacy invasion, and potential misuse. This research focuses on the development and evaluation of deep learning-based models such as CNN, RNN, RestNext and LSTM for detecting deepfake videos. Furthermore, we discuss potential challenges and future directions for deepfake detection research, emphasizing the importance of continued advancements in this field to stay ahead of evolving deepfake generation techniques. Ultimately, this research contributes to the ongoing efforts to combat the misuse of deepfake technology and safeguard the integrity of digital media and public discourse.

# Methodology

**A. Dataset**   
We are getting a dataset from Kaggle comprising three different datasets: FaceForensic++, Celeb-DF, and Deepfake Detection Challenge datasets (DFDC), which has been curated to augment the video data quantity. The collective dataset encompasses approximately 6,000 videos, evenly distributed into two categories: genuine (real) and manipulated (fake) videos. For the purpose of model development, 70% of this dataset has been allocated for training, while the remaining 30% is earmarked for testing and evaluation.

**B. Preprocessing**
Prior to model training, the videos undergo preprocessing. This involves face detection followed by precise face cropping to focus on facial features. This preprocessing step ensures that the model works with relevant data.
1)	Frame Extraction:
Initially, the video data is subjected to frame extraction, where each video is broken down into its constituent frames. This step is essential for further analysis.

2)	Face detection and cropping:
After frame extraction, a face detection algorithm is applied to identify and locate faces within each frame. Subsequently, the frames are cropped to isolate the detected faces. This is crucial for focusing the analysis on facial features.

3)	Uniform frame Quantity:
To maintain consistency in the number of frames across the dataset, the mean number of frames across all videos is computed. Subsequently, a new processed dataset is generated by retaining only the frames equal to this computed mean. This ensures uniformity in the dataset for training and evaluation purposes.

4)	Frame selection:
Frames that do not contain detectable faces are excluded during the preprocessing stage, ensuring that only frames with relevant facial content are considered.

5)	 Experimental frame limitations:
Given the computational intensity of processing a 10-second video at a rate of 30 frames per second, resulting in a total of 300 frames, it is proposed to limit the frame selection to the initial 100 frames for experimental purposes. This pragmatic choice is made to manage computational resources while still retaining a substantial amount of data for model training. This selection of the first 140 frames is deemed adequate for preliminary experiments and model development.

**C. Model** 
The model architecture employed for this study comprises a ResNeXt50_32x4d as the initial backbone, followed by a single Long Short-Term Memory (LSTM) layer. The Data Loader component of the system is responsible for loading the preprocessed face-cropped videos and performing a split to create separate training and testing datasets. Subsequently, the frames extracted from the preprocessed videos are fed into the model for both training and testing purposes, utilizing mini-batch processing to efficiently handle the data. This approach allows for the training and evaluation of the model on the provided video dataset.

**D. Feature Extraction**
Rather than creating a new classifier from scratch, we propose leveraging the ResNeXt Convolutional Neural Network (CNN) classifier to extract relevant features and achieve accurate frame-level feature detection. Subsequently, we intend to fine-tune this network by introducing additional necessary layers and optimizing the learning rate to ensure proper convergence of the gradient descent during model training. The 2049-dimensional feature vectors obtained after the final pooling layers of the ResNext network are then utilized as the input for the sequential Long Short-Term Memory (LSTM) layer, thereby preserving the critical information for further analysis and classification.

**E. Processing**
In our scenario, we are working with a sequence of ResNext CNN feature vectors, representing input frames, and the task at hand is binary classification, specifically discerning whether the sequence corresponds to a deepfake video or an unaltered one. A primary challenge we face is designing a model that can effectively process sequences in a coherent and meaningful manner. To address this challenge, we propose the use of a Long Short-Term Memory (LSTM) unit with 2049 memory cells, accompanied by a dropout rate of 0.4. This LSTM configuration is adept at fulfilling our objective.
LSTM is chosen for its capacity to process frames sequentially, enabling temporal analysis of the video. This analysis involves comparing the frame at time 't' seconds with frames from 't-n' seconds, where 'n' can vary, encompassing any number of frames preceding the 't' timestamp. The LSTM's sequential processing capability allows us to capture temporal dependencies and patterns, which are crucial for discerning between deepfake and genuine video sequences effectively.

**F. Model Architecture**
Our model architecture, represented in a coherent diagram, outlines the workflow for deepfake detection. The process begins with the 'Upload Video' phase, introducing the video into the system. This is followed by 'Video Processing,' which involves breaking down the video into individual frames for further analysis. The frames are then subjected to in-depth 'Processing,' where relevant features and data are extracted. The model, central to our system, takes on the crucial role of analysing these processed frames. Finally, based on the model's intricate analysis, the video is categorized as 'Real' or 'Fake.' This streamlined model architecture ensures not only efficiency but also robust defence against the proliferation of deceptive deepfake content.
 
**F. Predict**
In the prediction phase, when a novel video is presented to the pre-trained model for classification, a series of preprocessing steps is applied to the incoming video to align it with the format compatible with the trained model. This preprocessing workflow involves the segmentation of the video into individual frames, subsequent extraction of facial regions through cropping, and notably, the avoidance of local storage for the video content. Instead, the cropped frames are directly fed into the trained model for the purpose of deepfake detection. This direct pipeline from frame extraction to model input streamlines the inference process and enhances computational efficiency during real-time prediction.

# Result

The model's output will comprise a binary classification indicating whether the input video is a fake or a genuine (real) video, accompanied by a confidence score reflecting the model's level of certainty regarding the classification. A demonstrative instance of this output is visually represented in Figure for clarity and reference.

